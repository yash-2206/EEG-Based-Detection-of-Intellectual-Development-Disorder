{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe714b7f-8bd8-4637-8377-051fdd20b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import skew, kurtosis\n",
    "from mne.io import read_raw_edf\n",
    "from mne import make_fixed_length_epochs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import pywt\n",
    "from scipy.stats import entropy  \n",
    "from scipy.signal import coherence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ded780-38f5-4b26-8895-ba419f7af64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataverse_files\\\\h01.edf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_file_path=glob('dataverse_files/*.edf')\n",
    "all_file_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5497c315-aadb-4fa4-8634-37c0f1fef5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    healthy_file_path=[i for i in all_file_path if 'h' in i.split('\\\\')[1]] # split healthy patients files\n",
    "    patient_file_path=[i for i in all_file_path if 's' in i.split('\\\\')[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1684dcc2-8afa-4ced-bbe2-cc7649f642ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\test\\PR_EEG\\dataverse_files\\h01.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 231249  =      0.000 ...   924.996 secs...\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "Not setting metadata\n",
      "37 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 37 events and 6250 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37, 19, 6250)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(file_path):\n",
    "    datax=mne.io.read_raw_edf(file_path,preload=True)\n",
    "    datax.set_eeg_reference()# by default reference is average of all channels \n",
    "    datax.filter(l_freq=1,h_freq=45)\n",
    "    epochs=mne.make_fixed_length_epochs(datax,duration=25,overlap=0) # break the contious signal into smaller signals called epochs\n",
    "    epochs=epochs.get_data()\n",
    "    return epochs #no_of_trials,channels,length_of_signal\n",
    "\n",
    "data=read_data(healthy_file_path[0])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae8ba4c-63a1-4e91-831a-9d2ac97709d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "control_epochs_array=[read_data(subject) for subject in healthy_file_path]\n",
    "patients_epochs_array=[read_data(subject) for subject in patient_file_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c7e156-03f1-4c36-882f-593ba37b7481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 19, 6250)\n",
      "14 14\n"
     ]
    }
   ],
   "source": [
    "print(control_epochs_array[0].shape)\n",
    "control_epochs_labels=[len(i)*[0] for i in control_epochs_array]\n",
    "patients_epochs_labels=[len(i)*[1] for i in patients_epochs_array]\n",
    "print(len(control_epochs_labels),len(patients_epochs_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be292333-710b-4d8a-9cc1-5dd1db019aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n"
     ]
    }
   ],
   "source": [
    "data_list=control_epochs_array+patients_epochs_array\n",
    "label_list=control_epochs_labels+patients_epochs_labels\n",
    "print(len(data_list),len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7425fcc8-0b20-48b5-a503-d7c42941cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_list=[[i]*len(j) for i, j in enumerate(data_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7743c139-d6c6-46cd-8466-1e3405800ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1142, 19, 6250) (1142,) (1142,)\n"
     ]
    }
   ],
   "source": [
    "data_array=np.vstack(data_list)\n",
    "label_array=np.hstack(label_list)\n",
    "group_array=np.hstack(groups_list)\n",
    "print(data_array.shape,label_array.shape,group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f6f6579-dc1a-44b0-8f62-13b615ca7f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0db90502194dba8a6566d2eab534fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hurst import compute_Hc\n",
    "\n",
    "def mean(data):\n",
    "    return np.mean(data, axis=-1)\n",
    "\n",
    "def std(data):\n",
    "    return np.std(data, axis=-1)\n",
    "\n",
    "def ptp(data):\n",
    "    return np.ptp(data, axis=-1)\n",
    "\n",
    "def var(data):\n",
    "    return np.var(data, axis=-1)\n",
    "\n",
    "def minim(data):\n",
    "    return np.min(data, axis=-1)\n",
    "\n",
    "def maxim(data):\n",
    "    return np.max(data, axis=-1)\n",
    "\n",
    "def argminim(data):\n",
    "    return np.argmin(data, axis=-1)\n",
    "\n",
    "def argmaxim(data):\n",
    "    return np.argmax(data, axis=-1)\n",
    "\n",
    "def mean_square(data):\n",
    "    return np.mean(data**2, axis=-1)\n",
    "\n",
    "def rms(data):\n",
    "    return np.sqrt(np.mean(data**2, axis=-1))\n",
    "\n",
    "def abs_diffs_signal(data):\n",
    "    return np.sum(np.abs(np.diff(data, axis=-1)), axis=-1)\n",
    "\n",
    "def skewness(data):\n",
    "    return skew(data, axis=-1)\n",
    "\n",
    "def kurtosis_custom(data):\n",
    "    \"\"\"Compute kurtosis along the last axis manually.\"\"\"\n",
    "    if data.ndim == 2:  # Handles 2D data\n",
    "        return np.array([kurtosis(data[i, :]) for i in range(data.shape[0])])\n",
    "    elif data.ndim == 3:  # Handles 3D data\n",
    "        return np.array([[kurtosis(data[i, j, :]) for j in range(data.shape[1])] for i in range(data.shape[0])])\n",
    "    else:  # Handles 1D data\n",
    "        return kurtosis(data)\n",
    "\n",
    "\n",
    "def concatenate_features(data):\n",
    "    return np.concatenate((\n",
    "        mean(data), std(data), ptp(data), var(data), minim(data),\n",
    "        maxim(data), argminim(data), argmaxim(data),\n",
    "        mean_square(data), rms(data), abs_diffs_signal(data),\n",
    "        skewness(data), kurtosis_custom(data)\n",
    "    ), axis=-1)\n",
    "\n",
    "\n",
    "# Frequency band power\n",
    "def band_power(data, sf, band):\n",
    "    \"\"\"\n",
    "    Calculate the power of a specific frequency band for multi-channel EEG data.\n",
    "    Parameters:\n",
    "    - data: np.ndarray, shape (n_channels, n_samples)\n",
    "    - sf: Sampling frequency\n",
    "    - band: tuple, (fmin, fmax)\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray, shape (n_channels,) Band power for each channel\n",
    "    \"\"\"\n",
    "    fmin, fmax = band\n",
    "    psd_all = []\n",
    "    freqs_all = None\n",
    "\n",
    "    for channel in data:  # Loop through each channel\n",
    "        psd, freqs = welch(channel, sf, nperseg=256)\n",
    "        if freqs_all is None:  # Capture frequencies only once\n",
    "            freqs_all = freqs\n",
    "        psd_all.append(psd)\n",
    "\n",
    "    psd_all = np.array(psd_all)  # Shape: (n_channels, n_freqs)\n",
    "    idx_band = np.logical_and(freqs_all >= fmin, freqs_all <= fmax)\n",
    "    return np.sum(psd_all[:, idx_band], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_more_features(data, sf):\n",
    "    \"\"\"Extract additional features from signal data.\"\"\"\n",
    "    delta = band_power(data, sf, (1, 4))\n",
    "    theta = band_power(data, sf, (4, 8))\n",
    "    alpha = band_power(data, sf, (8, 13))\n",
    "    beta = band_power(data, sf, (13, 30))\n",
    "    gamma = band_power(data, sf, (30, 45))\n",
    "    return np.concatenate((delta, theta, alpha, beta, gamma), axis=-1)\n",
    "\n",
    "# --- Advanced Feature Extraction ---\n",
    "def wavelet_features(data):\n",
    "    coeffs = pywt.wavedec(data, wavelet='db4', level=4, axis=-1)\n",
    "    features = []\n",
    "    for coeff in coeffs:\n",
    "        features.append(np.mean(coeff, axis=-1))\n",
    "        features.append(np.std(coeff, axis=-1))\n",
    "    return np.concatenate(features, axis=-1)\n",
    "\n",
    "def hurst_exponent(data):\n",
    "    # Calculate the Hurst exponent for each signal/channel\n",
    "    hurst_vals = np.array([compute_Hc(data[i])[0] for i in range(data.shape[0])])\n",
    "    return hurst_vals\n",
    "\n",
    "\n",
    "def spectral_entropy(data, sf):\n",
    "    psd, freqs = welch(data, sf, nperseg=256, axis=-1)\n",
    "    psd_norm = psd / np.sum(psd, axis=-1, keepdims=True)  # Normalize\n",
    "    if psd_norm.size == 0:\n",
    "        print(\"Spectral entropy: Empty PSD!\")\n",
    "        return np.zeros_like(data)  # Return zeros if empty\n",
    "    return entropy(psd_norm, axis=-1)  # Ensure it returns a 1D array\n",
    "\n",
    "\n",
    "def connectivity_features(data, sf):\n",
    "    n_channels = data.shape[0]\n",
    "    coherence_vals = []\n",
    "    for i in range(n_channels):\n",
    "        for j in range(i + 1, n_channels):\n",
    "            _, Cxy = coherence(data[i], data[j], sf, nperseg=256)\n",
    "            coherence_vals.append(np.mean(Cxy))\n",
    "    return np.array(coherence_vals)\n",
    "\n",
    "\n",
    "def concatenate_features_with_new(data, sf):\n",
    "    basic_features = concatenate_features(data)\n",
    "    band_features = extract_more_features(data, sf)\n",
    "    \n",
    "    # Check if features are empty or scalars and reshape them\n",
    "    wavelet_feats = wavelet_features(data)\n",
    "    if wavelet_feats.ndim == 0:\n",
    "        wavelet_feats = np.expand_dims(wavelet_feats, axis=-1)\n",
    "    \n",
    "    hurst_feats = hurst_exponent(data)\n",
    "    if hurst_feats.ndim == 0:\n",
    "        hurst_feats = np.expand_dims(hurst_feats, axis=-1)\n",
    "    \n",
    "    entropy_feats = spectral_entropy(data, sf)\n",
    "    if entropy_feats.ndim == 0:\n",
    "        entropy_feats = np.expand_dims(entropy_feats, axis=-1)\n",
    "    \n",
    "    connectivity_feats = connectivity_features(data, sf)\n",
    "    if connectivity_feats.ndim == 0:\n",
    "        connectivity_feats = np.expand_dims(connectivity_feats, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    # Now concatenate them\n",
    "    return np.concatenate((basic_features, band_features, wavelet_feats, hurst_feats, entropy_feats, connectivity_feats), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sampling frequency\n",
    "sf = 250\n",
    "\n",
    "# Extract features\n",
    "expanded_features = []\n",
    "for data in tqdm(data_array):\n",
    "    expanded_features.append(concatenate_features_with_new(data, sf))\n",
    "expanded_features = np.array(expanded_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c19975b1-eb05-4615-a34c-12581bbb412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b9595b8-680b-43a0-9bb3-ec6a477a4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after PCA: (1142, 80)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(expanded_features)\n",
    "\n",
    "# Apply PCA\n",
    "n_components = 80  # Adjust based on desired explained variance\n",
    "pca = PCA(n_components=n_components)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "print(f\"Shape after PCA: {features_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1b859-2e9f-4411-a49a-41c84f4bb74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 900 candidates, totalling 4500 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'random_forest__n_estimators': [50, 100, 200, 300, 500],\n",
    "    'random_forest__max_depth': [None, 10, 20, 30, 40],\n",
    "    'random_forest__min_samples_split': [2, 5, 10],\n",
    "    'random_forest__min_samples_leaf': [1, 2, 5, 10],\n",
    "    'random_forest__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('random_forest', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Initialize GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "gscv = GridSearchCV(pipe, param_grid, cv=gkf, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search\n",
    "gscv.fit(features_pca, label_array, groups=group_array)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best parameters:\", gscv.best_params_)\n",
    "print(\"Best cross-validated accuracy:\", gscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934b438-b40e-4c98-a56e-147e510b138b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
